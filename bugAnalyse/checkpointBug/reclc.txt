$1: bf16[1, 3072] = torch._ops.aten.silu.default($0)
$3: bf16[3072, 18432] = torch._ops.aten.t.default($2)
$5: bf16[1, 18432] = torch._ops.aten.addmm.default($4, $1, $3)
$6: bf16[1, 3072] = torch._ops.aten.silu.default($0)
$8: bf16[3072, 18432] = torch._ops.aten.t.default($7)
$10: bf16[1, 18432] = torch._ops.aten.addmm.default($9, $6, $8)
['$11: bf16[1, 9216]', '$12: bf16[1, 9216]'] = torch._ops.aten.split.Tensor($5, 9216, -1)
['$13: bf16[1, 9216]', '$14: bf16[1, 9216]'] = torch._ops.aten.split.Tensor($10, 9216, -1)
$16: bf16[1, 6032, 3072] = torch._ops.aten.detach.default($15)
$17: bf16[1, 6032, 3072] = torch._ops.aten.detach.default($16)
('$18: bf16[1, 6032, 3072]', '$19: f32[1, 6032, 1]', '$20: f32[1, 6032, 1]') = torch._ops.aten.native_layer_norm.default($15, ['3072'], None, None, 1e-06)
['$21: bf16[1, 3072]', '$22: bf16[1, 3072]', '$23: bf16[1, 3072]'] = torch._ops.aten.split.Tensor($11, 3072, -1)
$24: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($22, 1)
$25: bf16[1, 1, 3072] = torch._ops.aten.add.Tensor($24, 1)
$26: bf16[1, 6032, 3072] = torch._ops.aten.mul.Tensor($18, $25)
$27: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($21, 1)
$28: bf16[1, 6032, 3072] = torch._ops.aten.add.Tensor($26, $27)
$29: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($23, 1)
$31: bf16[1, 116, 3072] = torch._ops.aten.detach.default($30)
$32: bf16[1, 116, 3072] = torch._ops.aten.detach.default($31)
('$33: bf16[1, 116, 3072]', '$34: f32[1, 116, 1]', '$35: f32[1, 116, 1]') = torch._ops.aten.native_layer_norm.default($30, ['3072'], None, None, 1e-06)
['$36: bf16[1, 3072]', '$37: bf16[1, 3072]', '$38: bf16[1, 3072]'] = torch._ops.aten.split.Tensor($13, 3072, -1)
$39: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($37, 1)
$40: bf16[1, 1, 3072] = torch._ops.aten.add.Tensor($39, 1)
$41: bf16[1, 116, 3072] = torch._ops.aten.mul.Tensor($33, $40)
$42: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($36, 1)
$43: bf16[1, 116, 3072] = torch._ops.aten.add.Tensor($41, $42)
$44: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($38, 1)
$45: bf16[6032, 3072] = torch._ops.aten.view.default($28, ['6032', '3072'])
$47: bf16[3072, 3072] = torch._ops.aten.t.default($46)
$49: bf16[6032, 3072] = torch._ops.aten.addmm.default($48, $45, $47)
$50: bf16[1, 6032, 3072] = torch._ops.aten.view.default($49, ['1', '6032', '3072'])
$51: bf16[6032, 3072] = torch._ops.aten.view.default($28, ['6032', '3072'])
$53: bf16[3072, 3072] = torch._ops.aten.t.default($52)
$55: bf16[6032, 3072] = torch._ops.aten.addmm.default($54, $51, $53)
$56: bf16[1, 6032, 3072] = torch._ops.aten.view.default($55, ['1', '6032', '3072'])
$57: bf16[6032, 3072] = torch._ops.aten.view.default($28, ['6032', '3072'])
$59: bf16[3072, 3072] = torch._ops.aten.t.default($58)
$61: bf16[6032, 3072] = torch._ops.aten.addmm.default($60, $57, $59)
$62: bf16[1, 6032, 3072] = torch._ops.aten.view.default($61, ['1', '6032', '3072'])
$63: bf16[116, 3072] = torch._ops.aten.view.default($43, ['116', '3072'])
$65: bf16[3072, 3072] = torch._ops.aten.t.default($64)
$67: bf16[116, 3072] = torch._ops.aten.addmm.default($66, $63, $65)
$68: bf16[1, 116, 3072] = torch._ops.aten.view.default($67, ['1', '116', '3072'])
$69: bf16[116, 3072] = torch._ops.aten.view.default($43, ['116', '3072'])
$71: bf16[3072, 3072] = torch._ops.aten.t.default($70)
$73: bf16[116, 3072] = torch._ops.aten.addmm.default($72, $69, $71)
$74: bf16[1, 116, 3072] = torch._ops.aten.view.default($73, ['1', '116', '3072'])
$75: bf16[116, 3072] = torch._ops.aten.view.default($43, ['116', '3072'])
$77: bf16[3072, 3072] = torch._ops.aten.t.default($76)
$79: bf16[116, 3072] = torch._ops.aten.addmm.default($78, $75, $77)
$80: bf16[1, 116, 3072] = torch._ops.aten.view.default($79, ['1', '116', '3072'])
$81: bf16[1, 6032, 24, 128] = torch._ops.aten.view.default($50, ['1', '6032', '24', '128'])
$82: bf16[1, 6032, 24, 128] = torch._ops.aten.view.default($56, ['1', '6032', '24', '128'])
$83: bf16[1, 6032, 24, 128] = torch._ops.aten.view.default($62, ['1', '6032', '24', '128'])
$84: bf16[1, 116, 24, 128] = torch._ops.aten.view.default($68, ['1', '116', '24', '128'])
$85: bf16[1, 116, 24, 128] = torch._ops.aten.view.default($74, ['1', '116', '24', '128'])
$86: bf16[1, 116, 24, 128] = torch._ops.aten.view.default($80, ['1', '116', '24', '128'])
$87: f32[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($81, dtype=torch.float32)
$88: f32[1, 6032, 24, 128] = torch._ops.aten.detach.default($87)
$89: f32[1, 6032, 24, 128] = torch._ops.aten.detach.default($88)
$90: f32[1, 6032, 24, 128] = torch._ops.aten.pow.Tensor_Scalar($87, 2)
$91: f32[1, 6032, 24, 1] = torch._ops.aten.mean.dim($90, ['-1'], True)
$92: f32[1, 6032, 24, 1] = torch._ops.aten.add.Tensor($91, 1e-06)
$93: f32[1, 6032, 24, 1] = torch._ops.aten.rsqrt.default($92)
$94: f32[1, 6032, 24, 1] = torch._ops.aten.detach.default($93)
$95: f32[1, 6032, 24, 1] = torch._ops.aten.detach.default($94)
$96: f32[1, 6032, 24, 1] = torch._ops.aten.detach.default($93)
$97: f32[1, 6032, 24, 1] = torch._ops.aten.detach.default($96)
$98: bf16[1, 6032, 24, 128] = torch._ops.aten.detach.default($81)
$99: bf16[1, 6032, 24, 128] = torch._ops.aten.detach.default($98)
$100: f32[1, 6032, 24, 128] = torch._ops.aten.mul.Tensor($81, $93)
$101: bf16[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($100, dtype=torch.bfloat16)
$103: bf16[1, 6032, 24, 128] = torch._ops.aten.mul.Tensor($101, $102)
$104: f32[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($82, dtype=torch.float32)
$105: f32[1, 6032, 24, 128] = torch._ops.aten.detach.default($104)
$106: f32[1, 6032, 24, 128] = torch._ops.aten.detach.default($105)
$107: f32[1, 6032, 24, 128] = torch._ops.aten.pow.Tensor_Scalar($104, 2)
$108: f32[1, 6032, 24, 1] = torch._ops.aten.mean.dim($107, ['-1'], True)
$109: f32[1, 6032, 24, 1] = torch._ops.aten.add.Tensor($108, 1e-06)
$110: f32[1, 6032, 24, 1] = torch._ops.aten.rsqrt.default($109)
$111: f32[1, 6032, 24, 1] = torch._ops.aten.detach.default($110)
$112: f32[1, 6032, 24, 1] = torch._ops.aten.detach.default($111)
$113: f32[1, 6032, 24, 1] = torch._ops.aten.detach.default($110)
$114: f32[1, 6032, 24, 1] = torch._ops.aten.detach.default($113)
$115: bf16[1, 6032, 24, 128] = torch._ops.aten.detach.default($82)
$116: bf16[1, 6032, 24, 128] = torch._ops.aten.detach.default($115)
$117: f32[1, 6032, 24, 128] = torch._ops.aten.mul.Tensor($82, $110)
$118: bf16[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($117, dtype=torch.bfloat16)
$120: bf16[1, 6032, 24, 128] = torch._ops.aten.mul.Tensor($118, $119)
$121: f32[1, 116, 24, 128] = torch._ops.aten._to_copy.default($84, dtype=torch.float32)
$122: f32[1, 116, 24, 128] = torch._ops.aten.detach.default($121)
$123: f32[1, 116, 24, 128] = torch._ops.aten.detach.default($122)
$124: f32[1, 116, 24, 128] = torch._ops.aten.pow.Tensor_Scalar($121, 2)
$125: f32[1, 116, 24, 1] = torch._ops.aten.mean.dim($124, ['-1'], True)
$126: f32[1, 116, 24, 1] = torch._ops.aten.add.Tensor($125, 1e-06)
$127: f32[1, 116, 24, 1] = torch._ops.aten.rsqrt.default($126)
$128: f32[1, 116, 24, 1] = torch._ops.aten.detach.default($127)
$129: f32[1, 116, 24, 1] = torch._ops.aten.detach.default($128)
$130: f32[1, 116, 24, 1] = torch._ops.aten.detach.default($127)
$131: f32[1, 116, 24, 1] = torch._ops.aten.detach.default($130)
$132: bf16[1, 116, 24, 128] = torch._ops.aten.detach.default($84)
$133: bf16[1, 116, 24, 128] = torch._ops.aten.detach.default($132)
$134: f32[1, 116, 24, 128] = torch._ops.aten.mul.Tensor($84, $127)
$135: bf16[1, 116, 24, 128] = torch._ops.aten._to_copy.default($134, dtype=torch.bfloat16)
$137: bf16[1, 116, 24, 128] = torch._ops.aten.mul.Tensor($135, $136)
$138: f32[1, 116, 24, 128] = torch._ops.aten._to_copy.default($85, dtype=torch.float32)
$139: f32[1, 116, 24, 128] = torch._ops.aten.detach.default($138)
$140: f32[1, 116, 24, 128] = torch._ops.aten.detach.default($139)
$141: f32[1, 116, 24, 128] = torch._ops.aten.pow.Tensor_Scalar($138, 2)
$142: f32[1, 116, 24, 1] = torch._ops.aten.mean.dim($141, ['-1'], True)
$143: f32[1, 116, 24, 1] = torch._ops.aten.add.Tensor($142, 1e-06)
$144: f32[1, 116, 24, 1] = torch._ops.aten.rsqrt.default($143)
$145: f32[1, 116, 24, 1] = torch._ops.aten.detach.default($144)
$146: f32[1, 116, 24, 1] = torch._ops.aten.detach.default($145)
$147: f32[1, 116, 24, 1] = torch._ops.aten.detach.default($144)
$148: f32[1, 116, 24, 1] = torch._ops.aten.detach.default($147)
$149: bf16[1, 116, 24, 128] = torch._ops.aten.detach.default($85)
$150: bf16[1, 116, 24, 128] = torch._ops.aten.detach.default($149)
$151: f32[1, 116, 24, 128] = torch._ops.aten.mul.Tensor($85, $144)
$152: bf16[1, 116, 24, 128] = torch._ops.aten._to_copy.default($151, dtype=torch.bfloat16)
$154: bf16[1, 116, 24, 128] = torch._ops.aten.mul.Tensor($152, $153)
$155: f32[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($103, dtype=torch.float32)
$156: f32[1, 6032, 24, 64, 2] = torch._ops.aten.view.default($155, ['1', '6032', '24', '-1', '2'])
$157: c64[1, 6032, 24, 64] = torch._ops.aten.view_as_complex.default($156)
$159: c64[6032, 1, 64] = torch._ops.aten.unsqueeze.default($158, 1)
$160: c64[1, 6032, 24, 64] = torch._ops.aten.mul.Tensor($157, $159)
$161: f32[1, 6032, 24, 64, 2] = torch._ops.aten.view_as_real.default($160)
$162: f32[1, 6032, 24, 128] = torch._ops.aten.view.default($161, ['1', '6032', '24', '128'])
$163: bf16[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($162, dtype=torch.bfloat16, layout=torch.strided, device=device(type='cuda', index=0))
$164: f32[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($120, dtype=torch.float32)
$165: f32[1, 6032, 24, 64, 2] = torch._ops.aten.view.default($164, ['1', '6032', '24', '-1', '2'])
$166: c64[1, 6032, 24, 64] = torch._ops.aten.view_as_complex.default($165)
$167: c64[6032, 1, 64] = torch._ops.aten.unsqueeze.default($158, 1)
$168: c64[1, 6032, 24, 64] = torch._ops.aten.mul.Tensor($166, $167)
$169: f32[1, 6032, 24, 64, 2] = torch._ops.aten.view_as_real.default($168)
$170: f32[1, 6032, 24, 128] = torch._ops.aten.view.default($169, ['1', '6032', '24', '128'])
$171: bf16[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($170, dtype=torch.bfloat16, layout=torch.strided, device=device(type='cuda', index=0))
$172: f32[1, 116, 24, 128] = torch._ops.aten._to_copy.default($137, dtype=torch.float32)
$173: f32[1, 116, 24, 64, 2] = torch._ops.aten.view.default($172, ['1', '116', '24', '-1', '2'])
$174: c64[1, 116, 24, 64] = torch._ops.aten.view_as_complex.default($173)
$176: c64[116, 1, 64] = torch._ops.aten.unsqueeze.default($175, 1)
$177: c64[1, 116, 24, 64] = torch._ops.aten.mul.Tensor($174, $176)
$178: f32[1, 116, 24, 64, 2] = torch._ops.aten.view_as_real.default($177)
$179: f32[1, 116, 24, 128] = torch._ops.aten.view.default($178, ['1', '116', '24', '128'])
$180: bf16[1, 116, 24, 128] = torch._ops.aten._to_copy.default($179, dtype=torch.bfloat16, layout=torch.strided, device=device(type='cuda', index=0))
$181: f32[1, 116, 24, 128] = torch._ops.aten._to_copy.default($154, dtype=torch.float32)
$182: f32[1, 116, 24, 64, 2] = torch._ops.aten.view.default($181, ['1', '116', '24', '-1', '2'])
$183: c64[1, 116, 24, 64] = torch._ops.aten.view_as_complex.default($182)
$184: c64[116, 1, 64] = torch._ops.aten.unsqueeze.default($175, 1)
$185: c64[1, 116, 24, 64] = torch._ops.aten.mul.Tensor($183, $184)
$186: f32[1, 116, 24, 64, 2] = torch._ops.aten.view_as_real.default($185)
$187: f32[1, 116, 24, 128] = torch._ops.aten.view.default($186, ['1', '116', '24', '128'])
$188: bf16[1, 116, 24, 128] = torch._ops.aten._to_copy.default($187, dtype=torch.bfloat16, layout=torch.strided, device=device(type='cuda', index=0))
$189: bf16[1, 6148, 24, 128] = torch._ops.aten.cat.default(['$180', '$163'], 1)
$190: bf16[1, 6148, 24, 128] = torch._ops.aten.cat.default(['$188', '$171'], 1)
$191: bf16[1, 6148, 24, 128] = torch._ops.aten.cat.default(['$86', '$83'], 1)
$192: bf16[1, 24, 6148, 128] = torch._ops.aten.permute.default($189, ['0', '2', '1', '3'])
$193: bf16[1, 24, 6148, 128] = torch._ops.aten.permute.default($190, ['0', '2', '1', '3'])
$194: bf16[1, 24, 6148, 128] = torch._ops.aten.permute.default($191, ['0', '2', '1', '3'])
$195: bf16[1, 24, 6148, 128] = torch._ops.aten.detach.default($193)
$196: bf16[1, 24, 6148, 128] = torch._ops.aten.detach.default($195)
$197: bf16[1, 24, 6148, 128] = torch._ops.aten.detach.default($192)
$198: bf16[1, 24, 6148, 128] = torch._ops.aten.detach.default($197)
$199: bf16[1, 24, 6148, 128] = torch._ops.aten.detach.default($194)
$200: bf16[1, 24, 6148, 128] = torch._ops.aten.detach.default($199)
('$201: bf16[1, 24, 6148, 128]', '$202: f32[1, 24, 6148]', 'None', 'None', '6148', '6148', '$203: u64[2]', '$204: u64[]', '$205: bf16[0]') = torch._ops.aten._scaled_dot_product_flash_attention.default($192, $193, $194, scale=0.08838834764831843)
$206: bf16[1, 24, 6148, 128] = torch._ops.aten.detach.default($201)
$207: bf16[1, 24, 6148, 128] = torch._ops.aten.detach.default($206)
$208: bf16[1, 6148, 24, 128] = torch._ops.aten.permute.default($201, ['0', '2', '1', '3'])
$209: bf16[1, 6148, 3072] = torch._ops.aten.view.default($208, ['1', '6148', '3072'])
$210: bf16[1, 6148, 3072] = torch._ops.aten.slice.Tensor($209, 0, 0, 9223372036854775807)
$211: bf16[1, 116, 3072] = torch._ops.aten.slice.Tensor($210, 1, 0, 116)
$212: bf16[1, 116, 3072] = torch._ops.aten.slice.Tensor($211, 2, 0, 9223372036854775807)
$213: bf16[1, 6148, 3072] = torch._ops.aten.slice.Tensor($209, 0, 0, 9223372036854775807)
$214: bf16[1, 6032, 3072] = torch._ops.aten.slice.Tensor($213, 1, 116, 9223372036854775807)
$215: bf16[1, 6032, 3072] = torch._ops.aten.slice.Tensor($214, 2, 0, 9223372036854775807)
$216: bf16[6032, 3072] = torch._ops.aten.view.default($215, ['6032', '3072'])
$218: bf16[3072, 3072] = torch._ops.aten.t.default($217)
$220: bf16[6032, 3072] = torch._ops.aten.addmm.default($219, $216, $218)
$221: bf16[1, 6032, 3072] = torch._ops.aten.view.default($220, ['1', '6032', '3072'])
$222: bf16[116, 3072] = torch._ops.aten.view.default($212, ['116', '3072'])
$224: bf16[3072, 3072] = torch._ops.aten.t.default($223)
$226: bf16[116, 3072] = torch._ops.aten.addmm.default($225, $222, $224)
$227: bf16[1, 116, 3072] = torch._ops.aten.view.default($226, ['1', '116', '3072'])
$228: bf16[1, 6032, 3072] = torch._ops.aten.mul.Tensor($29, $221)
$229: bf16[1, 6032, 3072] = torch._ops.aten.add.Tensor($15, $228)
$230: bf16[1, 116, 3072] = torch._ops.aten.mul.Tensor($44, $227)
$231: bf16[1, 116, 3072] = torch._ops.aten.add.Tensor($30, $230)
$232: bf16[1, 6032, 3072] = torch._ops.aten.detach.default($229)
$233: bf16[1, 6032, 3072] = torch._ops.aten.detach.default($232)
('$234: bf16[1, 6032, 3072]', '$235: f32[1, 6032, 1]', '$236: f32[1, 6032, 1]') = torch._ops.aten.native_layer_norm.default($229, ['3072'], None, None, 1e-06)
['$237: bf16[1, 3072]', '$238: bf16[1, 3072]', '$239: bf16[1, 3072]'] = torch._ops.aten.split.Tensor($12, 3072, -1)
$240: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($238, 1)
$241: bf16[1, 1, 3072] = torch._ops.aten.add.Tensor($240, 1)
$242: bf16[1, 6032, 3072] = torch._ops.aten.mul.Tensor($234, $241)
$243: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($237, 1)
$244: bf16[1, 6032, 3072] = torch._ops.aten.add.Tensor($242, $243)
$245: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($239, 1)
$246: bf16[6032, 3072] = torch._ops.aten.view.default($244, ['6032', '3072'])
$248: bf16[3072, 12288] = torch._ops.aten.t.default($247)
$250: bf16[6032, 12288] = torch._ops.aten.addmm.default($249, $246, $248)
$251: bf16[1, 6032, 12288] = torch._ops.aten.view.default($250, ['1', '6032', '12288'])
$252: bf16[1, 6032, 12288] = torch._ops.aten.detach.default($251)
$253: bf16[1, 6032, 12288] = torch._ops.aten.detach.default($252)
$254: bf16[1, 6032, 12288] = torch._ops.aten.gelu.default($251, approximate='tanh')
$255: bf16[6032, 12288] = torch._ops.aten.view.default($254, ['6032', '12288'])
$257: bf16[12288, 3072] = torch._ops.aten.t.default($256)
$259: bf16[6032, 3072] = torch._ops.aten.addmm.default($258, $255, $257)
$260: bf16[1, 6032, 3072] = torch._ops.aten.view.default($259, ['1', '6032', '3072'])
$261: bf16[1, 6032, 3072] = torch._ops.aten.mul.Tensor($245, $260)
$262: bf16[1, 6032, 3072] = torch._ops.aten.add.Tensor($229, $261)
$263: bf16[1, 116, 3072] = torch._ops.aten.detach.default($231)
$264: bf16[1, 116, 3072] = torch._ops.aten.detach.default($263)
('$265: bf16[1, 116, 3072]', '$266: f32[1, 116, 1]', '$267: f32[1, 116, 1]') = torch._ops.aten.native_layer_norm.default($231, ['3072'], None, None, 1e-06)
['$268: bf16[1, 3072]', '$269: bf16[1, 3072]', '$270: bf16[1, 3072]'] = torch._ops.aten.split.Tensor($14, 3072, -1)
$271: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($269, 1)
$272: bf16[1, 1, 3072] = torch._ops.aten.add.Tensor($271, 1)
$273: bf16[1, 116, 3072] = torch._ops.aten.mul.Tensor($265, $272)
$274: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($268, 1)
$275: bf16[1, 116, 3072] = torch._ops.aten.add.Tensor($273, $274)
$276: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($270, 1)
$277: bf16[116, 3072] = torch._ops.aten.view.default($275, ['116', '3072'])
$279: bf16[3072, 12288] = torch._ops.aten.t.default($278)
$281: bf16[116, 12288] = torch._ops.aten.addmm.default($280, $277, $279)
$282: bf16[1, 116, 12288] = torch._ops.aten.view.default($281, ['1', '116', '12288'])
$283: bf16[1, 116, 12288] = torch._ops.aten.detach.default($282)
$284: bf16[1, 116, 12288] = torch._ops.aten.detach.default($283)
$285: bf16[1, 116, 12288] = torch._ops.aten.gelu.default($282, approximate='tanh')
$286: bf16[116, 12288] = torch._ops.aten.view.default($285, ['116', '12288'])
$288: bf16[12288, 3072] = torch._ops.aten.t.default($287)
$290: bf16[116, 3072] = torch._ops.aten.addmm.default($289, $286, $288)
$291: bf16[1, 116, 3072] = torch._ops.aten.view.default($290, ['1', '116', '3072'])
$292: bf16[1, 116, 3072] = torch._ops.aten.mul.Tensor($276, $291)
$293: bf16[1, 116, 3072] = torch._ops.aten.add.Tensor($231, $292)