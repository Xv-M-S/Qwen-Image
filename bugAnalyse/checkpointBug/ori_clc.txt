$1: bf16[1, 3072] = torch._ops.aten.silu.default($0)
$3: bf16[3072, 18432] = torch._ops.aten.t.default($2)
$5: bf16[1, 18432] = torch._ops.aten.addmm.default($4, $1, $3)
$6: bf16[1, 3072] = torch._ops.aten.silu.default($0)
$8: bf16[3072, 18432] = torch._ops.aten.t.default($7)
$10: bf16[1, 18432] = torch._ops.aten.addmm.default($9, $6, $8)
['$11: bf16[1, 9216]', '$12: bf16[1, 9216]'] = torch._ops.aten.split.Tensor($5, 9216, -1)
['$13: bf16[1, 9216]', '$14: bf16[1, 9216]'] = torch._ops.aten.split.Tensor($10, 9216, -1)
('$16: bf16[1, 6032, 3072]', '$17: f32[1, 6032, 1]', '$18: f32[1, 6032, 1]') = torch._ops.aten.native_layer_norm.default($15, ['3072'], None, None, 1e-06)
['$19: bf16[1, 3072]', '$20: bf16[1, 3072]', '$21: bf16[1, 3072]'] = torch._ops.aten.split.Tensor($11, 3072, -1)
$22: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($20, 1)
$23: bf16[1, 1, 3072] = torch._ops.aten.add.Tensor($22, 1)
$24: bf16[1, 6032, 3072] = torch._ops.aten.mul.Tensor($16, $23)
$25: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($19, 1)
$26: bf16[1, 6032, 3072] = torch._ops.aten.add.Tensor($24, $25)
$27: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($21, 1)
('$29: bf16[1, 116, 3072]', '$30: f32[1, 116, 1]', '$31: f32[1, 116, 1]') = torch._ops.aten.native_layer_norm.default($28, ['3072'], None, None, 1e-06)
['$32: bf16[1, 3072]', '$33: bf16[1, 3072]', '$34: bf16[1, 3072]'] = torch._ops.aten.split.Tensor($13, 3072, -1)
$35: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($33, 1)
$36: bf16[1, 1, 3072] = torch._ops.aten.add.Tensor($35, 1)
$37: bf16[1, 116, 3072] = torch._ops.aten.mul.Tensor($29, $36)
$38: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($32, 1)
$39: bf16[1, 116, 3072] = torch._ops.aten.add.Tensor($37, $38)
$40: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($34, 1)
$41: bf16[6032, 3072] = torch._ops.aten.view.default($26, ['6032', '3072'])
$43: bf16[3072, 3072] = torch._ops.aten.t.default($42)
$45: bf16[6032, 3072] = torch._ops.aten.addmm.default($44, $41, $43)
$46: bf16[1, 6032, 3072] = torch._ops.aten.view.default($45, ['1', '6032', '3072'])
$47: bf16[6032, 3072] = torch._ops.aten.view.default($26, ['6032', '3072'])
$49: bf16[3072, 3072] = torch._ops.aten.t.default($48)
$51: bf16[6032, 3072] = torch._ops.aten.addmm.default($50, $47, $49)
$52: bf16[1, 6032, 3072] = torch._ops.aten.view.default($51, ['1', '6032', '3072'])
$53: bf16[6032, 3072] = torch._ops.aten.view.default($26, ['6032', '3072'])
$55: bf16[3072, 3072] = torch._ops.aten.t.default($54)
$57: bf16[6032, 3072] = torch._ops.aten.addmm.default($56, $53, $55)
$58: bf16[1, 6032, 3072] = torch._ops.aten.view.default($57, ['1', '6032', '3072'])
$59: bf16[116, 3072] = torch._ops.aten.view.default($39, ['116', '3072'])
$61: bf16[3072, 3072] = torch._ops.aten.t.default($60)
$63: bf16[116, 3072] = torch._ops.aten.addmm.default($62, $59, $61)
$64: bf16[1, 116, 3072] = torch._ops.aten.view.default($63, ['1', '116', '3072'])
$65: bf16[116, 3072] = torch._ops.aten.view.default($39, ['116', '3072'])
$67: bf16[3072, 3072] = torch._ops.aten.t.default($66)
$69: bf16[116, 3072] = torch._ops.aten.addmm.default($68, $65, $67)
$70: bf16[1, 116, 3072] = torch._ops.aten.view.default($69, ['1', '116', '3072'])
$71: bf16[116, 3072] = torch._ops.aten.view.default($39, ['116', '3072'])
$73: bf16[3072, 3072] = torch._ops.aten.t.default($72)
$75: bf16[116, 3072] = torch._ops.aten.addmm.default($74, $71, $73)
$76: bf16[1, 116, 3072] = torch._ops.aten.view.default($75, ['1', '116', '3072'])
$77: bf16[1, 6032, 24, 128] = torch._ops.aten.view.default($46, ['1', '6032', '24', '128'])
$78: bf16[1, 6032, 24, 128] = torch._ops.aten.view.default($52, ['1', '6032', '24', '128'])
$79: bf16[1, 6032, 24, 128] = torch._ops.aten.view.default($58, ['1', '6032', '24', '128'])
$80: bf16[1, 116, 24, 128] = torch._ops.aten.view.default($64, ['1', '116', '24', '128'])
$81: bf16[1, 116, 24, 128] = torch._ops.aten.view.default($70, ['1', '116', '24', '128'])
$82: bf16[1, 116, 24, 128] = torch._ops.aten.view.default($76, ['1', '116', '24', '128'])
$83: f32[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($77, dtype=torch.float32)
$84: f32[1, 6032, 24, 128] = torch._ops.aten.pow.Tensor_Scalar($83, 2)
$85: f32[1, 6032, 24, 1] = torch._ops.aten.mean.dim($84, ['-1'], True)
$86: f32[1, 6032, 24, 1] = torch._ops.aten.add.Tensor($85, 1e-06)
$87: f32[1, 6032, 24, 1] = torch._ops.aten.rsqrt.default($86)
$88: f32[1, 6032, 24, 128] = torch._ops.aten.mul.Tensor($77, $87)
$89: bf16[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($88, dtype=torch.bfloat16)
$91: bf16[1, 6032, 24, 128] = torch._ops.aten.mul.Tensor($89, $90)
$92: f32[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($78, dtype=torch.float32)
$93: f32[1, 6032, 24, 128] = torch._ops.aten.pow.Tensor_Scalar($92, 2)
$94: f32[1, 6032, 24, 1] = torch._ops.aten.mean.dim($93, ['-1'], True)
$95: f32[1, 6032, 24, 1] = torch._ops.aten.add.Tensor($94, 1e-06)
$96: f32[1, 6032, 24, 1] = torch._ops.aten.rsqrt.default($95)
$97: f32[1, 6032, 24, 128] = torch._ops.aten.mul.Tensor($78, $96)
$98: bf16[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($97, dtype=torch.bfloat16)
$100: bf16[1, 6032, 24, 128] = torch._ops.aten.mul.Tensor($98, $99)
$101: f32[1, 116, 24, 128] = torch._ops.aten._to_copy.default($80, dtype=torch.float32)
$102: f32[1, 116, 24, 128] = torch._ops.aten.pow.Tensor_Scalar($101, 2)
$103: f32[1, 116, 24, 1] = torch._ops.aten.mean.dim($102, ['-1'], True)
$104: f32[1, 116, 24, 1] = torch._ops.aten.add.Tensor($103, 1e-06)
$105: f32[1, 116, 24, 1] = torch._ops.aten.rsqrt.default($104)
$106: f32[1, 116, 24, 128] = torch._ops.aten.mul.Tensor($80, $105)
$107: bf16[1, 116, 24, 128] = torch._ops.aten._to_copy.default($106, dtype=torch.bfloat16)
$109: bf16[1, 116, 24, 128] = torch._ops.aten.mul.Tensor($107, $108)
$110: f32[1, 116, 24, 128] = torch._ops.aten._to_copy.default($81, dtype=torch.float32)
$111: f32[1, 116, 24, 128] = torch._ops.aten.pow.Tensor_Scalar($110, 2)
$112: f32[1, 116, 24, 1] = torch._ops.aten.mean.dim($111, ['-1'], True)
$113: f32[1, 116, 24, 1] = torch._ops.aten.add.Tensor($112, 1e-06)
$114: f32[1, 116, 24, 1] = torch._ops.aten.rsqrt.default($113)
$115: f32[1, 116, 24, 128] = torch._ops.aten.mul.Tensor($81, $114)
$116: bf16[1, 116, 24, 128] = torch._ops.aten._to_copy.default($115, dtype=torch.bfloat16)
$118: bf16[1, 116, 24, 128] = torch._ops.aten.mul.Tensor($116, $117)
$119: f32[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($91, dtype=torch.float32)
$120: f32[1, 6032, 24, 64, 2] = torch._ops.aten.view.default($119, ['1', '6032', '24', '-1', '2'])
$121: c64[1, 6032, 24, 64] = torch._ops.aten.view_as_complex.default($120)
$123: c64[6032, 1, 64] = torch._ops.aten.unsqueeze.default($122, 1)
$124: c64[1, 6032, 24, 64] = torch._ops.aten.mul.Tensor($121, $123)
$125: f32[1, 6032, 24, 64, 2] = torch._ops.aten.view_as_real.default($124)
$126: f32[1, 6032, 24, 128] = torch._ops.aten.view.default($125, ['1', '6032', '24', '128'])
$127: bf16[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($126, dtype=torch.bfloat16, layout=torch.strided, device=device(type='cuda', index=0))
$128: f32[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($100, dtype=torch.float32)
$129: f32[1, 6032, 24, 64, 2] = torch._ops.aten.view.default($128, ['1', '6032', '24', '-1', '2'])
$130: c64[1, 6032, 24, 64] = torch._ops.aten.view_as_complex.default($129)
$131: c64[6032, 1, 64] = torch._ops.aten.unsqueeze.default($122, 1)
$132: c64[1, 6032, 24, 64] = torch._ops.aten.mul.Tensor($130, $131)
$133: f32[1, 6032, 24, 64, 2] = torch._ops.aten.view_as_real.default($132)
$134: f32[1, 6032, 24, 128] = torch._ops.aten.view.default($133, ['1', '6032', '24', '128'])
$135: bf16[1, 6032, 24, 128] = torch._ops.aten._to_copy.default($134, dtype=torch.bfloat16, layout=torch.strided, device=device(type='cuda', index=0))
$136: f32[1, 116, 24, 128] = torch._ops.aten._to_copy.default($109, dtype=torch.float32)
$137: f32[1, 116, 24, 64, 2] = torch._ops.aten.view.default($136, ['1', '116', '24', '-1', '2'])
$138: c64[1, 116, 24, 64] = torch._ops.aten.view_as_complex.default($137)
$140: c64[116, 1, 64] = torch._ops.aten.unsqueeze.default($139, 1)
$141: c64[1, 116, 24, 64] = torch._ops.aten.mul.Tensor($138, $140)
$142: f32[1, 116, 24, 64, 2] = torch._ops.aten.view_as_real.default($141)
$143: f32[1, 116, 24, 128] = torch._ops.aten.view.default($142, ['1', '116', '24', '128'])
$144: bf16[1, 116, 24, 128] = torch._ops.aten._to_copy.default($143, dtype=torch.bfloat16, layout=torch.strided, device=device(type='cuda', index=0))
$145: f32[1, 116, 24, 128] = torch._ops.aten._to_copy.default($118, dtype=torch.float32)
$146: f32[1, 116, 24, 64, 2] = torch._ops.aten.view.default($145, ['1', '116', '24', '-1', '2'])
$147: c64[1, 116, 24, 64] = torch._ops.aten.view_as_complex.default($146)
$148: c64[116, 1, 64] = torch._ops.aten.unsqueeze.default($139, 1)
$149: c64[1, 116, 24, 64] = torch._ops.aten.mul.Tensor($147, $148)
$150: f32[1, 116, 24, 64, 2] = torch._ops.aten.view_as_real.default($149)
$151: f32[1, 116, 24, 128] = torch._ops.aten.view.default($150, ['1', '116', '24', '128'])
$152: bf16[1, 116, 24, 128] = torch._ops.aten._to_copy.default($151, dtype=torch.bfloat16, layout=torch.strided, device=device(type='cuda', index=0))
$153: bf16[1, 6148, 24, 128] = torch._ops.aten.cat.default(['$144', '$127'], 1)
$154: bf16[1, 6148, 24, 128] = torch._ops.aten.cat.default(['$152', '$135'], 1)
$155: bf16[1, 6148, 24, 128] = torch._ops.aten.cat.default(['$82', '$79'], 1)
$156: bf16[1, 24, 6148, 128] = torch._ops.aten.permute.default($153, ['0', '2', '1', '3'])
$157: bf16[1, 24, 6148, 128] = torch._ops.aten.permute.default($154, ['0', '2', '1', '3'])
$158: bf16[1, 24, 6148, 128] = torch._ops.aten.permute.default($155, ['0', '2', '1', '3'])
('$159: bf16[1, 24, 6148, 128]', '$160: f32[1, 24, 6148]', 'None', 'None', '6148', '6148', '$161: u64[2]', '$162: u64[]', '$163: bf16[0]') = torch._ops.aten._scaled_dot_product_flash_attention.default($156, $157, $158, scale=0.08838834764831843)
$164: bf16[1, 6148, 24, 128] = torch._ops.aten.permute.default($159, ['0', '2', '1', '3'])
$165: bf16[1, 6032, 116] = torch._ops.aten.empty.memory_format(['1', '6032', '116'], dtype=torch.bfloat16, device=device(type='cuda', index=0), pin_memory=False)
$166: bf16[1, 3072, 116] = torch._ops.aten.transpose.int($39, -1, -2)
$167: bf16[1, 6032, 116] = torch._ops.aten.baddbmm.default($165, $26, $166, beta=0, alpha=0.08838834764831845)
$168: bf16[1, 6032, 116] = torch._ops.aten._softmax.default($167, -1, False)
$169: bf16[1, 6148, 3072] = torch._ops.aten.view.default($164, ['1', '6148', '3072'])
$170: bf16[1, 6148, 3072] = torch._ops.aten.slice.Tensor($169, 0, 0, 9223372036854775807)
$171: bf16[1, 116, 3072] = torch._ops.aten.slice.Tensor($170, 1, 0, 116)
$172: bf16[1, 116, 3072] = torch._ops.aten.slice.Tensor($171, 2, 0, 9223372036854775807)
$173: bf16[1, 6148, 3072] = torch._ops.aten.slice.Tensor($169, 0, 0, 9223372036854775807)
$174: bf16[1, 6032, 3072] = torch._ops.aten.slice.Tensor($173, 1, 116, 9223372036854775807)
$175: bf16[1, 6032, 3072] = torch._ops.aten.slice.Tensor($174, 2, 0, 9223372036854775807)
$176: bf16[6032, 3072] = torch._ops.aten.view.default($175, ['6032', '3072'])
$178: bf16[3072, 3072] = torch._ops.aten.t.default($177)
$180: bf16[6032, 3072] = torch._ops.aten.addmm.default($179, $176, $178)
$181: bf16[1, 6032, 3072] = torch._ops.aten.view.default($180, ['1', '6032', '3072'])
$182: bf16[116, 3072] = torch._ops.aten.view.default($172, ['116', '3072'])
$184: bf16[3072, 3072] = torch._ops.aten.t.default($183)
$186: bf16[116, 3072] = torch._ops.aten.addmm.default($185, $182, $184)
$187: bf16[1, 116, 3072] = torch._ops.aten.view.default($186, ['1', '116', '3072'])
$188: bf16[1, 6032, 3072] = torch._ops.aten.mul.Tensor($27, $181)
$189: bf16[1, 6032, 3072] = torch._ops.aten.add.Tensor($15, $188)
$190: bf16[1, 116, 3072] = torch._ops.aten.mul.Tensor($40, $187)
$191: bf16[1, 116, 3072] = torch._ops.aten.add.Tensor($28, $190)
('$192: bf16[1, 6032, 3072]', '$193: f32[1, 6032, 1]', '$194: f32[1, 6032, 1]') = torch._ops.aten.native_layer_norm.default($189, ['3072'], None, None, 1e-06)
['$195: bf16[1, 3072]', '$196: bf16[1, 3072]', '$197: bf16[1, 3072]'] = torch._ops.aten.split.Tensor($12, 3072, -1)
$198: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($196, 1)
$199: bf16[1, 1, 3072] = torch._ops.aten.add.Tensor($198, 1)
$200: bf16[1, 6032, 3072] = torch._ops.aten.mul.Tensor($192, $199)
$201: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($195, 1)
$202: bf16[1, 6032, 3072] = torch._ops.aten.add.Tensor($200, $201)
$203: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($197, 1)
$204: bf16[6032, 3072] = torch._ops.aten.view.default($202, ['6032', '3072'])
$206: bf16[3072, 12288] = torch._ops.aten.t.default($205)
$208: bf16[6032, 12288] = torch._ops.aten.addmm.default($207, $204, $206)
$209: bf16[1, 6032, 12288] = torch._ops.aten.view.default($208, ['1', '6032', '12288'])
$210: bf16[1, 6032, 12288] = torch._ops.aten.gelu.default($209, approximate='tanh')
$211: bf16[6032, 12288] = torch._ops.aten.view.default($210, ['6032', '12288'])
$213: bf16[12288, 3072] = torch._ops.aten.t.default($212)
$215: bf16[6032, 3072] = torch._ops.aten.addmm.default($214, $211, $213)
$216: bf16[1, 6032, 3072] = torch._ops.aten.view.default($215, ['1', '6032', '3072'])
$217: bf16[1, 6032, 3072] = torch._ops.aten.mul.Tensor($203, $216)
$218: bf16[1, 6032, 3072] = torch._ops.aten.add.Tensor($189, $217)
('$219: bf16[1, 116, 3072]', '$220: f32[1, 116, 1]', '$221: f32[1, 116, 1]') = torch._ops.aten.native_layer_norm.default($191, ['3072'], None, None, 1e-06)
['$222: bf16[1, 3072]', '$223: bf16[1, 3072]', '$224: bf16[1, 3072]'] = torch._ops.aten.split.Tensor($14, 3072, -1)
$225: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($223, 1)
$226: bf16[1, 1, 3072] = torch._ops.aten.add.Tensor($225, 1)
$227: bf16[1, 116, 3072] = torch._ops.aten.mul.Tensor($219, $226)
$228: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($222, 1)
$229: bf16[1, 116, 3072] = torch._ops.aten.add.Tensor($227, $228)
$230: bf16[1, 1, 3072] = torch._ops.aten.unsqueeze.default($224, 1)
$231: bf16[116, 3072] = torch._ops.aten.view.default($229, ['116', '3072'])
$233: bf16[3072, 12288] = torch._ops.aten.t.default($232)
$235: bf16[116, 12288] = torch._ops.aten.addmm.default($234, $231, $233)
$236: bf16[1, 116, 12288] = torch._ops.aten.view.default($235, ['1', '116', '12288'])
$237: bf16[1, 116, 12288] = torch._ops.aten.gelu.default($236, approximate='tanh')
$238: bf16[116, 12288] = torch._ops.aten.view.default($237, ['116', '12288'])
$240: bf16[12288, 3072] = torch._ops.aten.t.default($239)
$242: bf16[116, 3072] = torch._ops.aten.addmm.default($241, $238, $240)
$243: bf16[1, 116, 3072] = torch._ops.aten.view.default($242, ['1', '116', '3072'])
$244: bf16[1, 116, 3072] = torch._ops.aten.mul.Tensor($230, $243)
$245: bf16[1, 116, 3072] = torch._ops.aten.add.Tensor($191, $244)